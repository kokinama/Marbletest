**Research Question**  

*How can a fully embedded, energy‑efficient active‑vision pipeline that combines heat‑map‑guided grasp detection, deterministic multi‑region “spring‑eye” view selection, and ultra‑light tactile slip detection be realized on a Xilinx Kria KV260 SoC, and to what extent does this integrated system improve grasp‑success rates, pose accuracy, and force‑adaptation for objects that are partially occluded, located at the image border, or have low‑friction surfaces, while keeping end‑to‑end latency below 50 ms and total power consumption under 5 W?*  

---

**Long Abstract**  

Robotic manipulation in unstructured environments demands fast, reliable perception and closed‑loop force control, yet most existing solutions rely on heavyweight GPUs or multiple cameras that are unsuitable for mobile platforms. This work presents a **complete, end‑to‑end manipulation pipeline that runs entirely on a single Xilinx Kria KV260 embedded AI board** (Zynq‑MPSoC with an on‑chip INT8 DPU and a Cortex‑R5 realtime core). The system integrates four tightly coupled components:

1. **Heat‑Map‑Guided Grasp Detection** – A ResNet‑34 backbone trained on the **GraspNet‑1 Billion** benchmark produces a three‑channel heat‑map (grasp centre, sine and cosine of the grasp angle). Quantisation‑aware training (QAT) yields an INT8 model (`.xmodel`) that executes on the DPU in ≈ 7 ms for a 640 × 480 RGB frame.

2. **Visuo‑Tactile Fusion via Mask‑and‑Replace** – The four corners of the image are masked and filled with a calibrated 8 × 8 pressure map from an I²C pressure matrix. Optional blurred‑edge patches (Gaussian‑blur of the full‑resolution image, down‑sampled and up‑sampled) provide low‑frequency contextual information without additional sensors.

3. **Active View Selection** – When the initial centre patch yields a low Grasp‑Score (a weighted combination of detection confidence, pose reprojection error, safety margin, and edge penalty), the pipeline automatically generates four overlapping patches (left, right, up, down). Each patch is processed by the DPU; the patch with the highest score is selected for grasp execution. This deterministic multi‑region scanning eliminates the need for mechanical pan‑tilt cameras while handling objects that lie near the image border or are partially occluded.

4. **Ultra‑Light Slip Detection & Adaptive Force Control** – A tiny convolutional network (two depth‑wise 3 × 3 convolutions + a fully‑connected layer, ≈ 1 k parameters) is trained on synthetic slip data generated with PyBullet and fine‑tuned on a small set of real grasps. After QAT, the model runs on the Cortex‑R5 in < 1 ms. Its binary slip flag drives a gain‑scheduled PID controller (1 kHz ISR) that raises the grasping force only when slip is detected, reducing peak contact forces on delicate objects by up to 30 % while preserving grasp stability.

All data exchange between the camera, pressure matrix, and the DPU is performed through a **DMA‑based ring buffer**, and inference jobs are queued to overlap CPU preprocessing with DPU execution, achieving an **end‑to‑end latency of ≤ 48 ms** (capture → motor command). Power profiling with an INA226 shows an average consumption of **< 5 W** (≈ 3 W for DPU inference, 0.8 W for the R5 core, and 0.5 W for peripherals).

The system is evaluated on a three‑fingered under‑actuated gripper using three representative object classes: a soft tomato, a rigid glass bottle, and a highly slippery mercury‑filled flask. Compared with a baseline vision‑only pipeline, the proposed architecture yields a **grasp‑success rate increase from 70 % to 92 %** for objects at the image border, a **pose error reduction from 4.2 mm to 3.5 mm**, and a **30 % reduction in peak contact force** for the soft object. Ablation studies confirm that each component (tactile fusion, spring‑eye scanning, blurred edge context) contributes significantly to the overall performance (p < 0.01, Cohen’s d ≈ 0.9).

To foster reproducibility, the complete source code (KV260 firmware, training scripts, Docker build environment), the calibrated pressure‑matrix driver, and a curated subset of synchronized RGB‑pressure recordings are released under a BSD‑3‑Clause license on GitHub, with a permanent Zenodo DOI for the dataset. This work demonstrates that **high‑performance, adaptive grasping can be achieved on a low‑power embedded platform**, opening the door to truly autonomous mobile manipulators in real‑world settings.